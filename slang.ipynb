{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d182dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb147de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Docker image tensorflow/tensorflow:2.14.0-gpu-juptyer\n",
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a959c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61904ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The device_name will be used in model.fit()\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpu_devices:\n",
    "    print(\"GPU available, using GPU\")\n",
    "    tf.config.experimental.set_visible_devices(gpu_devices[0], \"GPU\")\n",
    "    device_name = \"/GPU:0\"\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "    device_name = \"/CPU:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3f7e8",
   "metadata": {},
   "source": [
    "### Function for loading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load a text file and return an array of lines from the file.\n",
    "\n",
    "    Args:\n",
    "        file_path: str: The path to the file to load.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: An array of lines from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021dff4b-6b86-412c-ae8f-fe4a099fa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_slang(text):\n",
    "    slang_dict = {\"brb\": \"be right back\", \"lol\": \"laughing out loud\"}\n",
    "    return ' '.join(slang_dict.get(word, word) for word in text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:  \n",
    "        return [line.strip() for line in file]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751e373",
   "metadata": {},
   "source": [
    "### Load the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ff3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path: str = \"/Users/xuqianlong/Downloads/train_text.txt\"\n",
    "train_label_path: str = \"/Users/xuqianlong/Downloads/train_labels.txt\"\n",
    "\n",
    "test_text_path: str = \"/Users/xuqianlong/Downloads/test_text.txt\"\n",
    "test_label_path: str = \"/Users/xuqianlong/Downloads/test_labels.txt\"\n",
    "\n",
    "validationt_text_path: str = \"/Users/xuqianlong/Downloads/val_text.txt\"\n",
    "validationt_label_path: str = \"/Users/xuqianlong/Downloads/val_labels.txt\"\n",
    "\n",
    "train_text: list[str] = load_and_process_text_file(train_text_path)\n",
    "train_label: list[str] = load_and_process_text_file(train_label_path)\n",
    "\n",
    "test_text: list[str] = load_and_process_text_file(test_text_path)\n",
    "test_label: list[str] = load_and_process_text_file(test_label_path)\n",
    "\n",
    "validation_text: list[str] = load_and_process_text_file(validationt_text_path)\n",
    "validation_label: list[str] = load_and_process_text_file(validationt_label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd336c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da2e83",
   "metadata": {},
   "source": [
    "#### Download the NLTK resources and declere global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc75d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stopwords\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "nltk.download(\"vader_lexicon\", quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Declare a dict of emojis and their corresponding sentiment\n",
    "# 0: Negative; 1: Neutral; 2: Positive\n",
    "emoji_dict = {\n",
    "    \"ðŸ˜Š\": 2,\n",
    "    \"ðŸ˜‚\": 2,\n",
    "    \"ðŸ˜­\": 0,\n",
    "    \"ðŸ˜\": 2,\n",
    "    \"ðŸ˜˜\": 2,\n",
    "    \"ðŸ˜\": 2,\n",
    "    \"ðŸ˜©\": 0,\n",
    "    \"ðŸ˜\": 2,\n",
    "    \"ðŸ˜‰\": 2,\n",
    "    \"ðŸ˜Ž\": 2,\n",
    "    \"ðŸ˜¢\": 0,\n",
    "    \"ðŸ˜…\": 2,\n",
    "    \"ðŸ˜±\": 0,\n",
    "    \"ðŸ˜†\": 2,\n",
    "    \"ðŸ˜‹\": 2,\n",
    "    \"ðŸ˜·\": 0,\n",
    "    \"ðŸ˜”\": 0,\n",
    "    \"ðŸ˜’\": 0,\n",
    "    \"ðŸ˜¡\": 0,\n",
    "    \"ðŸ˜ª\": 0,\n",
    "    \"ðŸ˜¤\": 0,\n",
    "    \"ðŸ˜\": 2,\n",
    "    \"ðŸ˜“\": 0,\n",
    "    \"ðŸ˜–\": 0,\n",
    "    \"ðŸ˜£\": 0,\n",
    "    \"ðŸ˜ž\": 0,\n",
    "    \"ðŸ˜\": 1,\n",
    "    \"ðŸ˜•\": 0,\n",
    "    \"ðŸ˜«\": 0,\n",
    "    \"ðŸ˜¨\": 0,\n",
    "    \"ðŸ˜Œ\": 2,\n",
    "    \"ðŸ˜œ\": 2,\n",
    "    \"ðŸ˜‘\": 1,\n",
    "    \"ðŸ˜¬\": 0,\n",
    "    \"ðŸ˜ˆ\": 0,\n",
    "    \"ðŸ˜¯\": 0,\n",
    "    \"ðŸ˜³\": 0,\n",
    "    \"ðŸ˜‡\": 2,\n",
    "    \"ðŸ˜·\": 0,\n",
    "    \"ðŸ˜´\": 0,\n",
    "    \"ðŸ˜²\": 0,\n",
    "    \"ðŸ˜µ\": 0,\n",
    "    \"ðŸ˜¦\": 0,\n",
    "    \"ðŸ˜¢\": 0,\n",
    "    \"ðŸ˜®\": 0,\n",
    "    \"ðŸ˜Ÿ\": 0,\n",
    "    \"ðŸ˜¥\": 0,\n",
    "    \"ðŸ˜§\": 0,\n",
    "    \"ðŸ˜°\": 0,\n",
    "    \"ðŸ˜“\": 0,\n",
    "    \"ðŸ˜©\": 0,\n",
    "    \"ðŸ˜¿\": 0,\n",
    "    \"ðŸ˜¾\": 0,\n",
    "    \"ðŸ™€\": 0,\n",
    "    \"ðŸ™…\": 0,\n",
    "    \"ðŸ™†\": 0,\n",
    "    \"ðŸ™‡\": 0,\n",
    "    \"ðŸ™ˆ\": 0,\n",
    "    \"ðŸ™‰\": 0,\n",
    "    \"ðŸ™Š\": 0,\n",
    "    \"ðŸ™‹\": 0,\n",
    "    \"ðŸ™Œ\": 0,\n",
    "    \"ðŸ™\": 0,\n",
    "    \"ðŸ™Ž\": 0,\n",
    "    \"ðŸ™\": 0,\n",
    "    \":)\": 2,\n",
    "    \":(\": 0,\n",
    "    \"â¤ï¸\": 2,\n",
    "    \"ðŸ‘\": 2,\n",
    "    \"âœŒðŸ¼ï¸\": 2,\n",
    "    \"â˜¹ï¸\": 0,\n",
    "    \"ðŸ™ƒ\": 0,\n",
    "    \"ðŸ‘Ž\": 0,\n",
    "    \"ðŸ’™\": 2,\n",
    "    \"ðŸ’—\": 2,\n",
    "    \"ðŸŽ‰\": 2,\n",
    "    \"ðŸ˜„\": 2,\n",
    "    \"ðŸ¤—\": 2,\n",
    "    \":D\": 2,\n",
    "    \"ðŸŽ„\": 2,\n",
    "    \"ðŸŽ\": 2,\n",
    "    \":/\": 0,\n",
    "    \"?!\": 0,\n",
    "    \":P\": 2,\n",
    "    \":p\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4ecc5",
   "metadata": {},
   "source": [
    "### Functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text: list[str], data: str) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Process text data:\n",
    "    - Remove '@user'\n",
    "    - Remove hashtags\n",
    "    - Remove '-'\n",
    "    - Remove URLs\n",
    "    - Replace emojis with their corresponding sentiment.\n",
    "    - Replace punctuation marks.\n",
    "    - Tokenize the text.\n",
    "    - Normalize the text with nltk.\n",
    "    - Lowercase the text.\n",
    "    # Length of the line is added after vectorization.\n",
    "\n",
    "    Args:\n",
    "        text: list[str]: A list of text data.\n",
    "        data: str: The type of data (train, test, validation).\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: A list of list of processed text data.\n",
    "    \"\"\"\n",
    "    processed_text = []\n",
    "    for line in text:\n",
    "        # Remove '@user'\n",
    "        line = line.replace(\"@user\", \" \")\n",
    "        # Remove hashtags\n",
    "        line = line.replace(\"#\", \" \")\n",
    "        # Remove '-'\n",
    "        line = line.replace(\"-\", \" \")\n",
    "        # Remove URLs (http, https, www)\n",
    "        line = \" \".join([word for word in line.split() if \"http\" not in word])\n",
    "        line = \" \".join([word for word in line.split() if \"www\" not in word])\n",
    "        # Replace emojis with their corresponding sentiment\n",
    "        line = replace_emojis(line)\n",
    "        # Replace punctuation marks\n",
    "        line = punctuation_replacement(line)\n",
    "        # Tokenize the text.\n",
    "        tokens = nltk.word_tokenize(line)\n",
    "        # Normalize the text using WordNetLemmatizer and tokenize the text\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        # Lowercase the text\n",
    "        tokens = [str(word).lower() for word in tokens]\n",
    "        processed_text.append(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def vectorize_text(\n",
    "    input: list[list[str]], vocabulary: list[str]\n",
    ") -> ndarray[ndarray[float]]:\n",
    "    \"\"\"\n",
    "    Vectorize the text data.\n",
    "\n",
    "    Args:\n",
    "        input: list[list[str]]: A list of list of text data (Full *_text).\n",
    "        vocabulary: list[str]: The list of most common words.\n",
    "\n",
    "    Returns:\n",
    "        ndarray[ndarray[float]]: A numpy array of vectorized text data.\n",
    "    \"\"\"\n",
    "    vectorized_text = np.zeros((len(input), len(vocabulary)))\n",
    "    for i, line in enumerate(input):\n",
    "        for word in line:\n",
    "            if word in vocabulary:\n",
    "                vectorized_text[i, vocabulary.index(word)] += 1\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "def map_emoji_sentiment(input: int) -> str:\n",
    "    \"\"\"\n",
    "    Map the emoji sentiment to a string.\n",
    "\n",
    "    Args:\n",
    "        input: int: The emoji sentiment.\n",
    "\n",
    "    Returns:\n",
    "        str: The string sentiment.\n",
    "    \"\"\"\n",
    "    if input == 0:\n",
    "        return \" bad \"\n",
    "    elif input == 1:\n",
    "        return \" neutral \"\n",
    "    elif input == 2:\n",
    "        return \" good \"\n",
    "    else:\n",
    "        return \" neutral \"\n",
    "\n",
    "\n",
    "def replace_emojis(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace emojis with their corresponding sentiment.\n",
    "    If the emoji is 0, replace it with 'bad'.\n",
    "    If the emoji is 1, replace it with 'neutral'.\n",
    "    If the emoji is 2, replace it with 'good'.\n",
    "    If the emoji is not in the emoji_dict, replace it with 'neutral'.\n",
    "\n",
    "    Args:\n",
    "        input: str: The input text data (line).\n",
    "\n",
    "    Returns:\n",
    "        str: The text data with emojis replaced with their corresponding sentiment.\n",
    "    \"\"\"\n",
    "    for emoji in emoji_dict:\n",
    "        if emoji in input:\n",
    "            input = input.replace(emoji, map_emoji_sentiment(emoji_dict[emoji]))\n",
    "    return input\n",
    "\n",
    "\n",
    "def get_sentiment_score(line: list[str]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Get the sentiment score of the input text data from SentimentIntensityAnalyzer.\n",
    "\n",
    "    Args:\n",
    "        line: list[str]: The input line.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The compound score of 10 words in the sentence which has\n",
    "        the most significant score (far from 0). If the sentence has less than\n",
    "        10 words, the value of the remaining elements will be 0.\n",
    "        After that, the score for the whole line is added to the list.\n",
    "    \"\"\"\n",
    "    scores: list[float] = []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for string in line:\n",
    "        score = analyzer.polarity_scores(string)[\"compound\"]\n",
    "        scores.append(score)\n",
    "    line_score = analyzer.polarity_scores(\" \".join(line))[\"compound\"]\n",
    "\n",
    "    # Get the 10 most significant scores and add the line score\n",
    "    if len(scores) > 10:\n",
    "        scores.sort(key=lambda x: abs(x), reverse=True)\n",
    "        result = scores[:10]\n",
    "        result.append(line_score)\n",
    "    else:\n",
    "        result = scores\n",
    "        result += [0] * (10 - len(scores))\n",
    "        result.append(line_score)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def learning_rate_scheduler(epoch: int) -> float:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler, to decrease the learning rate when epoch increases.\n",
    "\n",
    "    Args:\n",
    "        epoch: int: The current epoch.\n",
    "\n",
    "    Returns:\n",
    "        float: The new learning rate.\n",
    "    \"\"\"\n",
    "    if epoch < 10:\n",
    "        return 0.0005\n",
    "    elif epoch < 20:\n",
    "        return 0.0003\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "\n",
    "def punctuation_replacement(line: Union[str, list[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Check if there are repeated (>= 2) punctuation marks ['.' '!', '?'] in the line.\n",
    "    If there are, no matter how many times the punctuation mark is repeated,\n",
    "    replace it with ['MultiDot', 'MultiExclamation', 'MultiQuestion']\n",
    "    respectively.\n",
    "\n",
    "    Replace ['.' '!', '?'] with ['Dot', 'Exclamation', 'Question'] respectively.\n",
    "    If they are not repeated in the line, keep them as they are.\n",
    "\n",
    "    Args:\n",
    "        line: Union[str, list[str]]: The input line, which can be a string or a list of words.\n",
    "\n",
    "    Returns:\n",
    "        str: The line with punctuation marks replaced.\n",
    "    \"\"\"\n",
    "    # Split the line into words if it is a string\n",
    "    if type(line) is str:\n",
    "        line = line.split()\n",
    "    # Replace punctuation marks\n",
    "    for_append = []\n",
    "    for i, word in enumerate(line):\n",
    "        if word.count(\".\") >= 2:\n",
    "            line[i] = line[i].replace(\".\", \"\")\n",
    "            for_append.append(\"MultiDot\")\n",
    "        elif word.count(\"!\") >= 2:\n",
    "            line[i] = line[i].replace(\"!\", \"\")\n",
    "            for_append.append(\"MultiExclamation\")\n",
    "        elif word.count(\"?\") >= 2:\n",
    "            line[i] = line[i].replace(\"?\", \"\")\n",
    "            for_append.append(\"MultiQuestion\")\n",
    "        elif word.count(\".\") == 1:\n",
    "            line[i] = line[i].replace(\".\", \"\")\n",
    "            for_append.append(\"Dot\")\n",
    "        elif word.count(\"!\") == 1:\n",
    "            line[i] = line[i].replace(\"!\", \"\")\n",
    "            for_append.append(\"Exclamation\")\n",
    "        elif word.count(\"?\") == 1:\n",
    "            line[i] = line[i].replace(\"?\", \"\")\n",
    "            for_append.append(\"Question\")\n",
    "\n",
    "    line += for_append\n",
    "\n",
    "    # Join back the line\n",
    "    return \" \".join(line)\n",
    "\n",
    "def load_and_process_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    processed_lines = [translate_slang(line.strip()) for line in lines]\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed5bf3",
   "metadata": {},
   "source": [
    "### Process the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba10fa",
   "metadata": {},
   "source": [
    "#### Process all text data\n",
    "\n",
    "See function docstring from text_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text: list[list[str]] = text_processing(train_text, \"train\")\n",
    "test_text: list[list[str]] = text_processing(test_text, \"test\")\n",
    "validation_text: list[list[str]] = text_processing(validation_text, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734a099",
   "metadata": {},
   "source": [
    "#### Get the length of each input line\n",
    "\n",
    "Will be added as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_len: list[int] = [len(line) for line in train_text]\n",
    "test_text_len: list[int] = [len(line) for line in test_text]\n",
    "validation_text_len: list[int] = [len(line) for line in validation_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d2450",
   "metadata": {},
   "source": [
    "#### Remove empty lines after processing\n",
    "\n",
    "As there may exist lines with 0 words after removing words like stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index of lists in train_text and validation_text that are empty\n",
    "empty_index_train: list[int] = [i for i, x in enumerate(train_text) if not x]\n",
    "\n",
    "# Remove empty lists from train_text and validation_text, and corresponding labels\n",
    "train_text: list[list[str]] = [\n",
    "    train_text[i] for i in range(len(train_text)) if i not in empty_index_train\n",
    "]\n",
    "train_label: list[list[str]] = [\n",
    "    train_label[i] for i in range(len(train_label)) if i not in empty_index_train\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde05ecd",
   "metadata": {},
   "source": [
    "#### Find the most common words in the training data\n",
    "\n",
    "Will be used for vectorizing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680dad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency: dict[str, int] = {}\n",
    "for line in train_text:\n",
    "    for word in line:\n",
    "        if word in word_frequency:\n",
    "            word_frequency[word] += 1\n",
    "        else:\n",
    "            word_frequency[word] = 1\n",
    "\n",
    "vocabulary: list[str] = [\n",
    "    word\n",
    "    for word, _ in sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)[\n",
    "        :10000\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d1eb8",
   "metadata": {},
   "source": [
    "#### Get the sentiment score of the text data\n",
    "\n",
    "Will be used as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf84bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_score: ndarray[ndarray[float]] = np.array(\n",
    "    [get_sentiment_score(line) for line in train_text]\n",
    ")\n",
    "test_sentiment_score: ndarray[ndarray[float]] = np.array(\n",
    "    [get_sentiment_score(line) for line in test_text]\n",
    ")\n",
    "validation_sentiment_score: ndarray[ndarray[float]] = np.array(\n",
    "    [get_sentiment_score(line) for line in validation_text]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41216d7",
   "metadata": {},
   "source": [
    "##### Convert the labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e846af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label: list[int] = [int(label) for label in train_label]\n",
    "test_label: list[int] = [int(label) for label in test_label]\n",
    "validation_label: list[int] = [int(label) for label in validation_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e8593",
   "metadata": {},
   "source": [
    "### Vectorize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "train_vectorized: ndarray[ndarray[float]] = vectorize_text(train_text, vocabulary)\n",
    "test_vectorized: ndarray[ndarray[float]] = vectorize_text(test_text, vocabulary)\n",
    "validation_vectorized: ndarray[ndarray[float]] = vectorize_text(\n",
    "    validation_text, vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b0aaf",
   "metadata": {},
   "source": [
    "#### Remove entries with all 0 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the entries with all zeros in the vectorized data\n",
    "zero_index_train: ndarray[int] = np.where(~train_vectorized.any(axis=1))[0]\n",
    "\n",
    "print(\"Original training data size:\", len(train_vectorized))\n",
    "\n",
    "# Remove entries with all zeros in the vectorized data, and corresponding labels\n",
    "train_vectorized: ndarray[ndarray[float]] = np.delete(\n",
    "    train_vectorized, zero_index_train, axis=0\n",
    ")\n",
    "train_label: list[int] = [\n",
    "    train_label[i] for i in range(len(train_label)) if i not in zero_index_train\n",
    "]\n",
    "\n",
    "# Remove entries with all zeros in the sentiment score data\n",
    "train_sentiment_score: ndarray[ndarray[float]] = np.delete(\n",
    "    train_sentiment_score, zero_index_train, axis=0\n",
    ")\n",
    "\n",
    "# Convert the list of text lengths to numpy array\n",
    "train_text_len: ndarray[int] = np.array(train_text_len)\n",
    "\n",
    "# Remove entries with all zeros in the text length data\n",
    "train_text_len: ndarray[int] = np.delete(train_text_len, zero_index_train)\n",
    "\n",
    "print(\"Processed training data size:\", len(train_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00348677",
   "metadata": {},
   "source": [
    "#### Add remaining feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52144176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the length of the text data to the vectorized data\n",
    "train_vectorized: ndarray[ndarray[float]] = np.column_stack(\n",
    "    (train_vectorized, train_text_len)\n",
    ")\n",
    "test_vectorized: ndarray[ndarray[float]] = np.column_stack(\n",
    "    (test_vectorized, test_text_len)\n",
    ")\n",
    "validation_vectorized: ndarray[ndarray[float]] = np.column_stack(\n",
    "    (validation_vectorized, validation_text_len)\n",
    ")\n",
    "\n",
    "# Add sentiment score to each entry in the vectorized data\n",
    "train_vectorized: ndarray[ndarray[float]] = np.concatenate(\n",
    "    (train_vectorized, train_sentiment_score), axis=1\n",
    ")\n",
    "test_vectorized: ndarray[ndarray[float]] = np.concatenate(\n",
    "    (test_vectorized, test_sentiment_score), axis=1\n",
    ")\n",
    "validation_vectorized: ndarray[ndarray[float]] = np.concatenate(\n",
    "    (validation_vectorized, validation_sentiment_score), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e326d3a",
   "metadata": {},
   "source": [
    "#### Convert labels to one-hot encoding\n",
    "\n",
    "For fitting into the neural network and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bea674",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_one_hot: ndarray[ndarray[float]] = tf.one_hot(train_label, 3)\n",
    "validation_label_one_hot: ndarray[ndarray[float]] = tf.one_hot(validation_label, 3)\n",
    "test_label_one_hot: ndarray[ndarray[float]] = tf.one_hot(test_label, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435cdbc",
   "metadata": {},
   "source": [
    "#### Final shape of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ac065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18faae",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdb1ad",
   "metadata": {},
   "source": [
    "#### Setup the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fea4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Input layer\n",
    "input_layer = tf.keras.layers.Input(shape=(train_vectorized.shape[1],))\n",
    "\n",
    "dropout_rate = 0.7\n",
    "activation_function = \"sigmoid\"\n",
    "\n",
    "#tf.random.set_seed(2024)\n",
    "\n",
    "# Define the model\n",
    "neural_network = tf.keras.models.Sequential(\n",
    "    [\n",
    "        input_layer,\n",
    "        tf.keras.layers.Dense(2048, activation=activation_function),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(512, activation=activation_function),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(128, activation=activation_function),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, mode=\"min\", verbose=1\n",
    ")\n",
    "\n",
    "# Add parameters to Adam optimizer if needed\n",
    "modified_adam = tf.keras.optimizers.Adam()\n",
    "\n",
    "neural_network.compile(\n",
    "    optimizer=modified_adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "rate_scheduler = tf.keras.callbacks.LearningRateScheduler(learning_rate_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0a3c4",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30804f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device_name):\n",
    "    history = neural_network.fit(\n",
    "        train_vectorized,\n",
    "        train_label_one_hot,\n",
    "        validation_data=(validation_vectorized, validation_label_one_hot),\n",
    "        epochs=200,\n",
    "        batch_size=512,\n",
    "        callbacks=[early_stopping, rate_scheduler],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa720ba6",
   "metadata": {},
   "source": [
    "#### Evaluate the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0da235",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = neural_network.evaluate(test_vectorized, test_label_one_hot)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04851f07",
   "metadata": {},
   "source": [
    "#### Plot graphs about loss and accuracy during epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e9916-5d05-451e-a047-655b43303c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural_network.save('63_test.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f066cdd-9b61-4ef3-a67a-a81d69926354",
   "metadata": {},
   "source": [
    "### For testing the model\n",
    "\n",
    "Will be deleted before pushing to main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3390c-5e30-40db-ae9e-a50afe44866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    input_layer = tf.keras.layers.Input(shape=(train_vectorized.shape[1],))\n",
    "    neural_network = tf.keras.models.Sequential(\n",
    "        [\n",
    "            input_layer,\n",
    "            tf.keras.layers.Dense(2048, activation=activation_function),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(512, activation=activation_function),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(128, activation=activation_function),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    neural_network.compile(\n",
    "        optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, mode=\"min\", verbose=1\n",
    "    )\n",
    "    with tf.device(device_name):\n",
    "        history = neural_network.fit(\n",
    "            train_vectorized,\n",
    "            train_label_one_hot,\n",
    "            validation_data=(validation_vectorized, validation_label_one_hot),\n",
    "            epochs=200,\n",
    "            batch_size=512,\n",
    "            callbacks=[early_stopping, rate_scheduler],\n",
    "        )\n",
    "    test_loss, test_accuracy = neural_network.evaluate(test_vectorized, test_label_one_hot)\n",
    "    print(i+1, test_accuracy)\n",
    "    accuracy_list.append(test_accuracy)\n",
    "    #if test_accuracy >= 0.641:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ae4d3-cdf0-40bc-bd6b-93a94ee41299",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb26f8d-0581-464d-a80d-70db54f3e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural_network.save('64_test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f254fe4-32ff-4912-8463-a5491fbe748a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a432dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0055d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eac577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
