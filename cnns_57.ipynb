{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K2nvqynYkMh",
        "outputId": "e899d662-edaa-4571-88e3-80171f4f3c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysastrawi in /usr/local/lib/python3.10/dist-packages (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as tf_hub\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install pysastrawi\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# !pip install pysastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt\"\n",
        "url_train_label = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt\"\n",
        "\n",
        "url_val_text = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt\"\n",
        "url_val_labels = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data = requests.get(url_train)\n",
        "trainlabel_data = requests.get(url_train_label)\n",
        "\n",
        "val_data = requests.get(url_val_text)\n",
        "vallabel_data = requests.get(url_val_labels)\n",
        "\n",
        "\n",
        "\n",
        "train_data1 = train_data.text.split(\"\\n\")\n",
        "\n",
        "train_data1_label = trainlabel_data.text.split(\"\\n\")\n",
        "\n",
        "val_data1 = val_data.text.split(\"\\n\")\n",
        "val_data1_label = vallabel_data.text.split(\"\\n\")\n",
        "\n",
        "\n",
        "# for line in data:\n",
        "# print(train_data1[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "kYiebT2XbmE4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Define stopwords for English\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "stemmer_en = PorterStemmer()\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    # Case folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Mention removal\n",
        "    text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "    # Hashtags removal\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "    # Newline removal (\\n)\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "\n",
        "    # Whitespace removal\n",
        "    text = text.strip()\n",
        "\n",
        "    # URL removal\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r\"www.\\S+\", \" \", text)\n",
        "\n",
        "    # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc)\n",
        "    text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopwords removal\n",
        "    tokens = [word for word in tokens if word not in stopwords_en]\n",
        "\n",
        "    # Stemming using Porter Stemmer\n",
        "    tokens = [stemmer_en.stem(word) for word in tokens]\n",
        "\n",
        "    # Remove 'th' characters\n",
        "    tokens = [word.replace('th', '') for word in tokens]\n",
        "\n",
        "    # Combining Tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "S9MeeBtU0cDz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_processning = [text_preprocessing(text) for text in train_data1]\n",
        "for lin in train_data_processning[:3]:\n",
        "  print(lin)\n",
        "print('\\n')\n",
        "# for lin in text_preprocessing[:3]:\n",
        "#   print(lin)\n",
        "\n",
        "val_data_processning = [text_preprocessing(text) for text in val_data1]\n"
      ],
      "metadata": {
        "id": "J9UYS9Mp0dSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf33954-844d-4124-93ab-bd3e70ef9dc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qt origin draft  book remu lupin surviv battl hogwart\n",
            "ben smi smi concuss remain lineup ursday curti\n",
            "sorri bout stream last night crash tonight sure back minecraft pc tomorrow night\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for lin in train_data1[:5]:\n",
        "  print(lin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWKXiiqF7B4t",
        "outputId": "7fce9b30-6258-42bb-e15a-9ab98dbd5f9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\" \n",
            "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\" \n",
            "Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night. \n",
            "Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays \n",
            "@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Extracting text from training and test data\n",
        "# X_train_texts = [sample[0] for sample in X_train]\n",
        "# X_test_texts = [sample[0] for sample in X_test]\n",
        "\n",
        "# Initialise the CountVectorizer and vectorise it.\n",
        "Vectorize = CountVectorizer()\n",
        "X_train_vec = Vectorize.fit_transform(train_data_processning)\n",
        "X_test_vec = Vectorize.transform(val_data_processning)\n",
        "\n",
        "# Finding the Number of Vocabs and Max Token Length in One Document\n",
        "total_vocab = len(Vectorize.vocabulary_.keys())\n",
        "max_sen_len = max([len(i.split(\" \")) for i in train_data_processning])\n",
        "\n",
        "print('Total Vocab : ', total_vocab)\n",
        "print('Maximum Sentence Length : ', max_sen_len, 'tokens')\n",
        "\n",
        "# Text Vectorization\n",
        "text_vectorization = TextVectorization(max_tokens=total_vocab,\n",
        "                                       standardize=\"lower_and_strip_punctuation\",\n",
        "                                       split=\"whitespace\",\n",
        "                                       ngrams=None,\n",
        "                                       output_mode=\"int\",\n",
        "                                       output_sequence_length=max_sen_len,\n",
        "                                       input_shape=(1,)) # Only use in Sequential API\n",
        "\n",
        "text_vectorization.adapt(train_data_processning)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G32SlIwSYlc_",
        "outputId": "ef69ae3f-02f5-43f4-f8f8-a859f4a18d6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Vocab :  27215\n",
            "Maximum Sentence Length :  40 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## Document example\n",
        "# print('Document example')\n",
        "# print(df.text_processed[0])\n",
        "# print('')\n",
        "\n",
        "# ## Result of Text Vectorization\n",
        "# print('Result of Text Vectorization')\n",
        "# print(text_vectorization([df.text_processed[0]]))\n",
        "# print('Vector size : ', text_vectorization([df.text_processed[0]]).shape)\n",
        "\n",
        "\n",
        "\n",
        "## Document example\n",
        "print('Document example')\n",
        "print(train_data_processning[0])\n",
        "print('')\n",
        "\n",
        "## Result of Text Vectorization\n",
        "print('Result of Text Vectorization')\n",
        "# Vectorise the first training data text\n",
        "text_vectorized = text_vectorization([train_data_processning[0]])\n",
        "print(text_vectorized)\n",
        "print('Vector size : ', text_vectorized.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PH5rhQna0r-",
        "outputId": "93c5aac7-069a-428e-bfdb-9fcf282621c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document example\n",
            "qt origin draft  book remu lupin surviv battl hogwart\n",
            "\n",
            "Result of Text Vectorization\n",
            "tf.Tensor(\n",
            "[[ 8910   919  1216   217  5936  6014  2008  1331 12093     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]], shape=(1, 40), dtype=int64)\n",
            "Vector size :  (1, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the Top 20 Tokens (Sorted by the Highest Frequency of Appearance)\n",
        "\n",
        "text_vectorization.get_vocabulary()[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfrUsa8pa2vf",
        "outputId": "1f309ef3-0208-4642-ccf8-7a8a8745a34c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'u',\n",
              " 's',\n",
              " 'tomorrow',\n",
              " 'may',\n",
              " 'go',\n",
              " 'c',\n",
              " 'day',\n",
              " 'nt',\n",
              " 'st',\n",
              " 'night',\n",
              " 'see',\n",
              " 'get',\n",
              " 'friday',\n",
              " 'time',\n",
              " 'like',\n",
              " 'sunday',\n",
              " 'm',\n",
              " 'saturday']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_processed = [sentence.split() for sentence in train_data_processning]\n",
        "print(train_data_processed[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DRGLXtXXiil",
        "outputId": "13776996-62a5-4959-e786-3c4cf2eb39eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['qt', 'origin', 'draft', 'book', 'remu', 'lupin', 'surviv', 'battl', 'hogwart'], ['ben', 'smi', 'smi', 'concuss', 'remain', 'lineup', 'ursday', 'curti']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "# model = Word2Vec(train_data_processed, vector_size=100, window=5, min_count=3)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=train_data_processed, vector_size=128, window=5, min_count=1, workers=4)\n",
        "word2vec_model.train(train_data_processed, total_examples=len(train_data_processed), epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th1ihSJbzmzn",
        "outputId": "8e73e01f-3ea0-440b-9d44-a8bbc38ed0bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23905988, 26264550)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_data_processning)\n",
        "\n",
        "# Convert text to sequence\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_data_processning)\n",
        "X_val_seq = tokenizer.texts_to_sequences(val_data_processning)\n",
        "\n",
        "\n",
        "# cleaning the invalide data\n",
        "invalid_indices_train = [i for i, seq in enumerate(X_train_seq) if len(seq) == 0]\n",
        "invalid_indices_val = [i for i, seq in enumerate(X_val_seq) if len(seq) == 0]\n",
        "\n",
        "X_train_seq_clean = [seq for i, seq in enumerate(X_train_seq) if i not in invalid_indices_train]\n",
        "y_train_ohe_clean = np.delete(train_data1_label, invalid_indices_train, axis=0)\n",
        "\n",
        "\n",
        "X_val_seq_clean = [seq for i, seq in enumerate(X_val_seq) if i not in invalid_indices_val]\n",
        "y_val_ohe_clean = np.delete(val_data1_label, invalid_indices_val, axis=0)\n",
        "\n",
        "# update index\n",
        "max_seq_length = max(len(seq) for seq in X_train_seq_clean + X_val_seq_clean)\n",
        "X_train_padded_clean = pad_sequences(X_train_seq_clean, maxlen=max_seq_length)\n",
        "X_val_padded_clean = pad_sequences(X_val_seq_clean, maxlen=max_seq_length)\n",
        "\n",
        "# update\n",
        "y_train_ohe_clean = to_categorical(y_train_ohe_clean)\n",
        "y_val_ohe_clean = to_categorical(y_val_ohe_clean)\n",
        "\n",
        "print(\"train data after cleaning:\", len(X_train_seq_clean))\n",
        "print(\"develop(val) data after cleaning:\", len(X_val_seq_clean))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_3a0TO1xtcv",
        "outputId": "a2a44c1e-7615-4fe5-ea3c-3440fee20936"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data after cleaning: 45614\n",
            "develop(val) data after cleaning: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get the pre-trained word embeddings\n",
        "word_embeddings = word2vec_model.wv\n",
        "\n",
        "# Get the embedding matrix\n",
        "embedding_matrix = word_embeddings.vectors\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(word_embeddings.key_to_index)\n",
        "\n",
        "# Update the embedding layer in your neural network model\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=128,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_seq_length,\n",
        "                            trainable=False)\n",
        "\n",
        "\n",
        "# Embedding\n",
        "\n",
        "\n",
        "# embedding = Embedding(input_dim=total_vocab,\n",
        "#                       output_dim=128,\n",
        "#                       embeddings_initializer=\"uniform\",\n",
        "#                       input_length=max_sen_len)\n",
        "# Example Result\n",
        "\n",
        "\n",
        "# print('Document example')\n",
        "# print(train_data_processning[0])\n",
        "# print('')\n",
        "\n",
        "# ## Result of Text Vectorization\n",
        "# print('Result of Text Vectorization')\n",
        "# print(text_vectorization([train_data_processning[0]]))\n",
        "# print('Vector size : ', text_vectorization([train_data_processning[0]]).shape)\n",
        "# print('')\n",
        "\n",
        "# ## Result of Embedding\n",
        "# print('Result of Embedding')\n",
        "# print(embedding(text_vectorization([train_data_processning[0]])))\n",
        "# print('Vector size : ', embedding(text_vectorization([train_data_processning[0]])).shape)\n"
      ],
      "metadata": {
        "id": "RzmHmN8YyIew"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# learn_rate = tf.keras.optimizers.Adam(learning_rate =0.001)\n",
        "\n",
        "# model_lstm_1 = Sequential()\n",
        "# model_lstm_1.add(embedding_layer)\n",
        "# # model_lstm_1.add(Embedding(input_dim=10000, output_dim=128, input_length=max_seq_length))\n",
        "# model_lstm_1.add(Bidirectional(LSTM(8, return_sequences=True, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42))))\n",
        "# model_lstm_1.add(BatchNormalization())\n",
        "# model_lstm_1.add(Dropout(0.1))\n",
        "# model_lstm_1.add(Bidirectional(LSTM(16, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42))))\n",
        "# model_lstm_1.add(BatchNormalization())\n",
        "# model_lstm_1.add(Dropout(0.1))\n",
        "# # model_lstm_1.add(Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "# model_lstm_1.add(Dense(3, activation='softmax'))\n",
        "# model_lstm_1.compile(loss='categorical_crossentropy', optimizer=learn_rate, metrics=['accuracy'])\n",
        "# model_lstm_1.summary()\n",
        "# # Fit the model\n",
        "# # batch_size =32\n",
        "# batch_size =32\n",
        "# model_lstm_1_hist = model_lstm_1.fit(X_train_padded_clean,\n",
        "#                                      y_train_ohe_clean,\n",
        "#                                      epochs=25,\n",
        "#                                      batch_size=batch_size,\n",
        "#                                      validation_data=(X_val_padded_clean, y_val_ohe_clean))\n",
        "\n",
        "\n",
        "# # Plot training results\n",
        "# model_lstm_1_hist_df = pd.DataFrame(model_lstm_1_hist.history)\n",
        "\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# sns.lineplot(data=model_lstm_1_hist_df[['accuracy', 'val_accuracy']])\n",
        "# plt.grid()\n",
        "# plt.title('Accuracy vs Val-Accuracy')\n",
        "\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# sns.lineplot(data=model_lstm_1_hist_df[['loss', 'val_loss']])\n",
        "# plt.grid()\n",
        "# plt.title('Loss vs Val-Loss')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "EW7X8LsRpPrs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "embedding_dim = 100  # 假设设置为100维\n",
        "filters = 300\n",
        "#  reduce\n",
        "kernel_size = 5\n",
        "hidden_units = 64\n",
        "pool_size = 2\n",
        "dropout_rate = 0.5\n",
        "learning_rate = 0.001\n",
        "# 创建一个新的 Sequential 模型\n",
        "model_cnn_multiclass = Sequential()\n",
        "# max_length = 32\n",
        "\n",
        "model_cnn_multiclass.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length))\n",
        "model_cnn_multiclass.add(Dropout(0.4))\n",
        "# model_cnn_multiclass.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
        "model_cnn_multiclass.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_multiclass.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_multiclass.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_multiclass.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_multiclass.add(MaxPooling1D(pool_size=pool_size))\n",
        "model_cnn_multiclass.add(Flatten())\n",
        "model_cnn_multiclass.add(Dense(600))\n",
        "model_cnn_multiclass.add(Dropout(0.5))\n",
        "# model_cnn_multiclass.add(Dense(1)) binary\n",
        "model_cnn_multiclass.add(Dense(units=hidden_units, activation='relu'))\n",
        "# model_cnn_multiclass.add(Activation('softmax'))\n",
        "model_cnn_multiclass.add(Dropout(rate=dropout_rate))\n",
        "num_classes = 3\n",
        "model_cnn_multiclass.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model_cnn_multiclass.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# 打印模型概要信息\n",
        "model_cnn_multiclass.summary()\n",
        "\n",
        "# 训练模型\n",
        "batch_size =32\n",
        "\n",
        "history_cnn_multiclass = model_cnn_multiclass.fit(X_train_padded_clean,\n",
        "                                                  y_train_ohe_clean,\n",
        "                                                  epochs=25,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  validation_data=(X_val_padded_clean, y_val_ohe_clean),\n",
        "                                                  verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "# Plot training results\n",
        "model_lstm_1_hist_df_cnn = pd.DataFrame(history_cnn_multiclass.history)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.lineplot(data=model_lstm_1_hist_df_cnn[['accuracy', 'val_accuracy']])\n",
        "plt.grid()\n",
        "plt.title('Accuracy vs Val-Accuracy')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.lineplot(data=model_lstm_1_hist_df_cnn[['loss', 'val_loss']])\n",
        "plt.grid()\n",
        "plt.title('Loss vs Val-Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "opUkjAoXG6e5",
        "outputId": "693667f5-cdb8-488b-ae7e-c4406c70ceac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 32, 100)           2776400   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 32, 100)           0         \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 28, 300)           150300    \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 24, 300)           450300    \n",
            "                                                                 \n",
            " conv1d_16 (Conv1D)          (None, 20, 150)           225150    \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 16, 75)            56325     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 8, 75)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 600)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 600)               360600    \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 600)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                38464     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4057734 (15.48 MB)\n",
            "Trainable params: 4057734 (15.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "1426/1426 [==============================] - 210s 146ms/step - loss: 0.9016 - accuracy: 0.5593 - val_loss: 0.7850 - val_accuracy: 0.6535\n",
            "Epoch 2/25\n",
            "1426/1426 [==============================] - 208s 146ms/step - loss: 0.7581 - accuracy: 0.6692 - val_loss: 0.7703 - val_accuracy: 0.6610\n",
            "Epoch 3/25\n",
            "1426/1426 [==============================] - 208s 146ms/step - loss: 0.6827 - accuracy: 0.7112 - val_loss: 0.7980 - val_accuracy: 0.6565\n",
            "Epoch 4/25\n",
            "1426/1426 [==============================] - 209s 146ms/step - loss: 0.6192 - accuracy: 0.7467 - val_loss: 0.8076 - val_accuracy: 0.6380\n",
            "Epoch 5/25\n",
            "1426/1426 [==============================] - 213s 150ms/step - loss: 0.5595 - accuracy: 0.7748 - val_loss: 0.8651 - val_accuracy: 0.6480\n",
            "Epoch 6/25\n",
            " 728/1426 [==============>...............] - ETA: 1:39 - loss: 0.4868 - accuracy: 0.8109"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3551ffc667a4>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m history_cnn_multiclass = model_cnn_multiclass.fit(X_train_padded_clean, \n\u001b[0m\u001b[1;32m     47\u001b[0m                                                   \u001b[0my_train_ohe_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcMSImJj4w1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_index = model_lstm_1_hist_df['val_accuracy'].idxmax()\n",
        "best_model = model_lstm_1_hist_df.iloc[best_model_index]\n",
        "print(\"Best Model Epoch:\", best_model_index + 1)\n",
        "print(\"Validation Accuracy:\", best_model['val_accuracy'])\n",
        "print(\"Validation Loss:\", best_model['val_loss'])\n",
        "\n",
        "# the best model information\n",
        "print(\"\\nBest Model Summary:\")\n",
        "best_model_summary = model_lstm_1.to_json()\n",
        "print(best_model_summary)\n",
        "# saving\n",
        "\n",
        "# best_model.save(\"best_model_lstm_1.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_OYOjUsX-1k",
        "outputId": "96d3867d-bb89-42f1-a1ef-f40c5a5b8316"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Epoch: 11\n",
            "Validation Accuracy: 0.6539999842643738\n",
            "Validation Loss: 0.7935364842414856\n",
            "\n",
            "Best Model Summary:\n",
            "{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 32], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"embedding_input\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"Embedding\", \"config\": {\"name\": \"embedding\", \"trainable\": false, \"dtype\": \"float32\", \"batch_input_shape\": [null, 32], \"input_dim\": 27764, \"output_dim\": 128, \"embeddings_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"RandomUniform\", \"config\": {\"minval\": -0.05, \"maxval\": 0.05, \"seed\": null}, \"registered_name\": null}, \"embeddings_regularizer\": null, \"activity_regularizer\": null, \"embeddings_constraint\": null, \"mask_zero\": false, \"input_length\": 32}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"Bidirectional\", \"config\": {\"name\": \"bidirectional\", \"trainable\": true, \"dtype\": \"float32\", \"layer\": {\"module\": \"keras.layers\", \"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm\", \"trainable\": true, \"dtype\": \"float32\", \"return_sequences\": true, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"time_major\": false, \"units\": 8, \"activation\": \"tanh\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": 42}, \"registered_name\": null}, \"recurrent_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 2}, \"registered_name\": null}, \"merge_mode\": \"concat\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32, 128]}}, {\"module\": \"keras.layers\", \"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization\", \"trainable\": true, \"dtype\": \"float32\", \"axis\": [2], \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"gamma_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Ones\", \"config\": {}, \"registered_name\": null}, \"moving_mean_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"moving_variance_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Ones\", \"config\": {}, \"registered_name\": null}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32, 16]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0.1, \"noise_shape\": null, \"seed\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32, 16]}}, {\"module\": \"keras.layers\", \"class_name\": \"Bidirectional\", \"config\": {\"name\": \"bidirectional_1\", \"trainable\": true, \"dtype\": \"float32\", \"layer\": {\"module\": \"keras.layers\", \"class_name\": \"LSTM\", \"config\": {\"name\": \"lstm_1\", \"trainable\": true, \"dtype\": \"float32\", \"return_sequences\": false, \"return_state\": false, \"go_backwards\": false, \"stateful\": false, \"unroll\": false, \"time_major\": false, \"units\": 16, \"activation\": \"tanh\", \"recurrent_activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": 42}, \"registered_name\": null}, \"recurrent_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Orthogonal\", \"config\": {\"gain\": 1.0, \"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"unit_forget_bias\": true, \"kernel_regularizer\": null, \"recurrent_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"recurrent_constraint\": null, \"bias_constraint\": null, \"dropout\": 0.0, \"recurrent_dropout\": 0.0, \"implementation\": 2}, \"registered_name\": null}, \"merge_mode\": \"concat\"}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32, 16]}}, {\"module\": \"keras.layers\", \"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization_1\", \"trainable\": true, \"dtype\": \"float32\", \"axis\": [1], \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"gamma_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Ones\", \"config\": {}, \"registered_name\": null}, \"moving_mean_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"moving_variance_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Ones\", \"config\": {}, \"registered_name\": null}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dropout\", \"config\": {\"name\": \"dropout_1\", \"trainable\": true, \"dtype\": \"float32\", \"rate\": 0.1, \"noise_shape\": null, \"seed\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 3, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 32]}}]}, \"keras_version\": \"2.15.0\", \"backend\": \"tensorflow\"}\n"
          ]
        }
      ]
    }
  ]
}