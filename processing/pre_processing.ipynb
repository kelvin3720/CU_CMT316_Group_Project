{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYNBO8hzsjgR",
        "outputId": "1f4b2c41-6c27-4d10-bf46-06ffeaa5a6bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pysastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pysastrawi\n",
            "Successfully installed pysastrawi-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as tf_hub\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install pysastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# !pip install pysastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt\"\n",
        "url_train_label = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt\"\n",
        "\n",
        "url_val_text = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt\"\n",
        "url_val_labels = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data = requests.get(url_train)\n",
        "trainlabel_data = requests.get(url_train_label)\n",
        "\n",
        "val_data = requests.get(url_val_text)\n",
        "vallabel_data = requests.get(url_val_labels)\n",
        "\n",
        "\n",
        "\n",
        "train_data1 = train_data.text.split(\"\\n\")[:100]\n",
        "train_data1_label = trainlabel_data.text.split(\"\\n\")[:100]\n",
        "\n",
        "val_data1 = val_data.text.split(\"\\n\")[:100]\n",
        "val_data1_label = vallabel_data.text.split(\"\\n\")[:100]\n",
        "\n"
      ],
      "metadata": {
        "id": "28_Qswxtslmi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Stopwords\n",
        "\n",
        "stpwds_id = list(set(stopwords.words('english')))\n",
        "stpwds_id.append('oh') # Cara lain dari \"stpwds_id = stpwds_id + ['oh']\"\n",
        "\n",
        "# Define Stemming\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
        "  text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "  # Newline removal (\\n)\n",
        "  text = re.sub(r\"\\\\n\", \" \",text)\n",
        "  text = text.strip()\n",
        "  text = re.sub(r\"http\\S+\", \" \", text)\n",
        "  text = re.sub(r\"www.\\S+\", \" \", text)\n",
        "\n",
        "  # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
        "  text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [word for word in tokens if word not in stpwds_id]\n",
        "  tokens = [stemmer.stem(word) for word in tokens]\n",
        "  text = ' '.join(tokens)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "VWe1mtfOso4w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "train_data_processning = [text_preprocessing(text) for text in train_data1]\n",
        "for lin in train_data_processning:\n",
        "  print(lin)\n",
        "print('\\n')\n",
        "for lin in train_data1:\n",
        "  print(lin)\n",
        "\n",
        "val_data_processning = [text_preprocessing(text) for text in val_data1]\n"
      ],
      "metadata": {
        "id": "c4HdplIzs0Bx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}