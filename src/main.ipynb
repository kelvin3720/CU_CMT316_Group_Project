{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c054e6-4504-450d-8234-c9cae7b7aa0c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23273f6d-f11c-48fc-a7a0-56115f6ad4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fad0d2-bed8-4c40-b96e-73a4ab591b60",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da72e511-2021-4986-a2d2-78723d9b1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path: str) -> ndarray[str]:\n",
    "    \"\"\"\n",
    "    Load a text file and return an array of lines from the file.\n",
    "\n",
    "    Args:\n",
    "        file_path: str: The path to the file to load.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: An array of lines from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return np.array([line.strip() for line in lines])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3de75-20de-4bfe-a9c6-2c41b6dc5b39",
   "metadata": {},
   "source": [
    "### Load the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a9a7d4-b97b-471b-ae8b-1794810ceaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path: str = \"../dataset/train_text.txt\"\n",
    "train_label_path: str = \"../dataset/train_labels.txt\"\n",
    "\n",
    "test_text_path: str = \"../dataset/test_text.txt\"\n",
    "test_label_path: str = \"../dataset/test_text.txt\"\n",
    "\n",
    "validationt_text_path: str = \"../dataset/val_text.txt\"\n",
    "validationt_label_path: str = \"../dataset/val_text.txt\"\n",
    "\n",
    "train_text: ndarray[str] = load_text_file(train_text_path)\n",
    "train_label: ndarray[str] = load_text_file(train_label_path)\n",
    "\n",
    "test_text: ndarray[str] = load_text_file(test_text_path)\n",
    "test_label: ndarray[str] = load_text_file(test_label_path)\n",
    "\n",
    "validationt_text: ndarray[str] = load_text_file(validationt_text_path)\n",
    "validationt_label: ndarray[str] = load_text_file(validationt_label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e2122-1846-4b96-8a26-6ad82e682dab",
   "metadata": {},
   "source": [
    "## Kelvin's Part\n",
    "\n",
    "Imports will be moved to the top of this notebook only after the model is accepted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e3f0c-e3db-485d-b611-b12f4a2a2c74",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744ed3eb-f676-461b-bbad-af61558497b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f77297-92d1-4254-b44e-e731e2b55029",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f43a81-38a2-4361-9a84-753a0d9662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_tokens(sentence: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a sentence into words.\n",
    "    \n",
    "    Args:\n",
    "        sentence: a string.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokenized strings.\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    sentence_split = nltk.tokenize.sent_tokenize(sentence)\n",
    "    list_tokens = []\n",
    "    for substring in sentence_split:\n",
    "        list_tokens_sentence = nltk.tokenize.word_tokenize(substring)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "def get_vector_text(list_vocab: list[str], input_string: str) -> ndarray[float]:\n",
    "    \"\"\"\n",
    "    Generate a vector representation of the input string based on word frequency.\n",
    "\n",
    "    Args:\n",
    "        list_vocab: A list of vocabulary words.\n",
    "        input_string: The input string.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A NumPy array of float representing the vectorized text.\n",
    "    \"\"\"\n",
    "    vector_text: ndarray = np.zeros(len(list_vocab))\n",
    "    list_tokens_string: ndarray[str] = get_list_tokens(input_string)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i] = list_tokens_string.count(word)\n",
    "    return np.array(vector_text)\n",
    "\n",
    "def get_sentiment(line: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Get the score of a word based on sentiment analysis from nltk.\n",
    "\n",
    "    I (Kelvin) am not using the analyzer directly on the sentence because it\n",
    "    cannot demonstrate our expertise in applied machine learning.\n",
    "    I have asked the professor and he allowed us to use SentimentIntensityAnalyzer.\n",
    "\n",
    "    Args:\n",
    "        line: A sentence.\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: The compound score of 10 words in the sentence which has\n",
    "        the most significant score (far from 0). If the sentence has less than\n",
    "        10 words, the value of the remaining elements will be 0.\n",
    "        \n",
    "        The size of the list is always 10.\n",
    "    \"\"\"\n",
    "    scores: list[float] = []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for word in get_list_tokens(line):\n",
    "        score = analyzer.polarity_scores(word)\n",
    "        scores.append(score['compound'])\n",
    "    \n",
    "    retult = [0.0] * 10\n",
    "\n",
    "    # Get the 10 most significant scores\n",
    "    top_10: list[float] = sorted(scores, key=lambda x: abs(x), reverse=True)[:10]\n",
    "\n",
    "    # Add 0.0 if the length is less than 10\n",
    "    top_10 += [0.0] * (10 - len(top_10))\n",
    "\n",
    "    return top_10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed7c58e-fd0b-45a0-aa2a-b0679b608c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Create a set of stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.update({\".\", \",\", \"--\", \"``\", \"''\", \"@\", \"#\", \":\", \";\"})\n",
    "stopwords.update({\"&\", \"(\", \")\", \"-\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb578db-1668-4938-b8b5-698b136fec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ! - 16866\n",
      "2. user - 16855\n",
      "3. 's - 8444\n",
      "4. ... - 7467\n",
      "5. ? - 7240\n",
      "6. tomorrow - 7201\n",
      "7. may - 6636\n",
      "8. day - 4080\n",
      "9. n't - 3604\n",
      "10. wa - 3507\n",
      "11. night - 3044\n",
      "12. going - 3028\n",
      "13. see - 2842\n",
      "14. friday - 2697\n",
      "15. sunday - 2611\n",
      "16. 1st - 2609\n",
      "17. time - 2564\n",
      "18. like - 2507\n",
      "19. get - 2477\n",
      "20. 'm - 2221\n",
      "21. go - 2138\n",
      "22. saturday - 2101\n",
      "23. amp - 2013\n",
      "24. game - 1960\n",
      "25. one - 1960\n"
     ]
    }
   ],
   "source": [
    "# Create dict_word_frequency\n",
    "# word is the key, count of the word is the value\n",
    "dict_word_frequency: dict[str, int] = {}\n",
    "for line in train_text:\n",
    "    sentence_tokens = get_list_tokens(line)\n",
    "    for word in sentence_tokens:\n",
    "        if word in stopwords: continue\n",
    "        if word not in dict_word_frequency: dict_word_frequency[word] = 1\n",
    "        else: dict_word_frequency[word] += 1\n",
    "\n",
    "# Get the top 1000 most frequent words\n",
    "sorted_list: list[tuple[str, int]] = sorted(\n",
    "    dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True\n",
    "    )[:1000]\n",
    "\n",
    "# Show the top 25 most frequent words and their counts\n",
    "i = 0 # Reset i\n",
    "for word, frequency in sorted_list[:25]:\n",
    "    i += 1\n",
    "    print(f\"{str(i)}. {word} - {str(frequency)}\")\n",
    "\n",
    "vocabulary: list[str] = [word for word, _ in sorted_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "393ad9c0-b012-40c9-9c8b-86dd08a08cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i, line in enumerate(train_text):\n",
    "    # Feature 1: Word frequency\n",
    "    vector_pos: ndarray[float] = get_vector_text(vocabulary, line)\n",
    "    # Feature 2: Score from SentimentIntensityAnalyzer by nltk\n",
    "    # This takes around 4 min on my PC (i5-12400)\n",
    "    score: ndarray[float] = np.array(get_sentiment(line))\n",
    "    x_train.append(np.concatenate((score, vector_pos)))\n",
    "    y_train.append(train_label[i])\n",
    "\n",
    "# x_train is already a numpy array\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c22e9f-635d-4ee2-a5c6-adc71b6b31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45615\n",
      "45615\n",
      "[ 0.5106  0.4215 -0.3818  0.3182  0.      0.      0.      0.      0.\n",
      "  0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "  0.      0.    ]\n",
      "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"\n",
      "[ 0.3182 -0.0772  0.      0.      0.      0.      0.      0.      0.\n",
      "  0.      0.      0.      0.      0.      0.      1.      0.      0.\n",
      "  0.      0.    ]\n",
      "Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "for i in range (0, 4):\n",
    "    print(x_train[i][:20])\n",
    "    print(train_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae5fb9-a829-4715-bfea-18a6cb93817c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ce6c93-d423-40bc-98c3-c341d921227e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a699c3-27a9-40b1-9dcf-97a18e63e97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b5ace8-b15b-44c5-884c-75f8ccb950b0",
   "metadata": {},
   "source": [
    "### Result - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16abda82-01f9-4b21-80e2-714bcac9d391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd88942-e3c9-4a6e-8e7f-a7c4d2a9ce2b",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc54690-240a-4198-8834-e280fa355b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff3be8f-ea40-45ed-bc8e-f3b9fe7e25b1",
   "metadata": {},
   "source": [
    "### Result - K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8d284-54f4-4624-b001-07d77d468b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4816ef55-7669-4f24-9b7f-462038611d3c",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f4812-2955-446e-ac7d-c09bae52fb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11524783-eabb-431e-a528-fab55b5b7d25",
   "metadata": {},
   "source": [
    "### Result - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46992e16-21ad-42f8-b2c2-af3808a33ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f9c6336-a12b-45b3-8bf7-1690da58f1e1",
   "metadata": {},
   "source": [
    "## [Name]'s Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b04873-f24b-4c5f-9d4d-2ddb9c3100e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1e195a-f676-454b-acd8-6fd4fb63fffa",
   "metadata": {},
   "source": [
    "your markdown here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
