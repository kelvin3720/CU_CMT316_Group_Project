{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D6DS3wCcmct"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as tf_hub\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Dropout, Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install pysastrawi\n",
        "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# !pip install pysastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url_train = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_text.txt\"\n",
        "url_train_label = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/train_labels.txt\"\n",
        "\n",
        "url_val_text = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_text.txt\"\n",
        "url_val_labels = \"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/val_labels.txt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data = requests.get(url_train)\n",
        "trainlabel_data = requests.get(url_train_label)\n",
        "\n",
        "val_data = requests.get(url_val_text)\n",
        "vallabel_data = requests.get(url_val_labels)\n",
        "\n",
        "\n",
        "\n",
        "train_data1 = train_data.text.split(\"\\n\")\n",
        "\n",
        "train_data1_label = trainlabel_data.text.split(\"\\n\")\n",
        "\n",
        "val_data1 = val_data.text.split(\"\\n\")\n",
        "val_data1_label = vallabel_data.text.split(\"\\n\")\n",
        "\n",
        "\n",
        "# for line in data:\n",
        "# print(train_data1[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "QCae1L6ocoiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Define stopwords for English\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "stemmer_en = PorterStemmer()\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    # Case folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Mention removal\n",
        "    text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "    # Hashtags removal\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
        "\n",
        "    # Newline removal (\\n)\n",
        "    text = re.sub(r\"\\\\n\", \" \", text)\n",
        "\n",
        "    # Whitespace removal\n",
        "    text = text.strip()\n",
        "\n",
        "    # URL removal\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r\"www.\\S+\", \" \", text)\n",
        "\n",
        "    # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
        "    text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopwords removal\n",
        "    tokens = [word for word in tokens if word not in stopwords_en]\n",
        "\n",
        "    # Stemming using Porter Stemmer\n",
        "    tokens = [stemmer_en.stem(word) for word in tokens]\n",
        "\n",
        "    # Combining Tokens\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "vNqdYWAqcr8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_processning = [text_preprocessing(text) for text in train_data1]\n",
        "# for lin in train_data_processning:\n",
        "#   print(lin)\n",
        "# print('\\n')\n",
        "# for lin in train_data1:\n",
        "#   print(lin)\n",
        "\n",
        "val_data_processning = [text_preprocessing(text) for text in val_data1]\n"
      ],
      "metadata": {
        "id": "q23OJ9Sgcv8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}